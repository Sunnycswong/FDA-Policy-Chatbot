{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c30c12dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Library\n",
    "\n",
    "import openai\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import AzureDeveloperCliCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    HnswParameters,\n",
    "    PrioritizedFields,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SemanticSettings,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmConfiguration,\n",
    ")\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "import openai\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "import openai\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "import json\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "#from langchain.retrievers import AzureCognitiveSearchRetriever\n",
    "from langdetect import detect\n",
    "from langchain.prompts import PromptTemplate\n",
    "import re\n",
    "# Create chain to answer questions\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Import Azure OpenAI\n",
    "from langchain.llms import AzureOpenAI \n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "#import textwrap\n",
    "import logging\n",
    "\n",
    "# lingua\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "\n",
    "\"\"\"# setting up credentials\n",
    "os.environ[\"AZURE_COGNITIVE_SEARCH_SERVICE_NAME\"] = \"gptdemosearch\" # replace with yours search service name\n",
    "os.environ[\"AZURE_COGNITIVE_SEARCH_API_KEY\"] = \"PcAZcXbX2hJsxMYExc2SnkMFO0D94p7Zw3Qzeu5WjYAzSeDMuR5O\" # replace with your api key\n",
    "os.environ[\"AZURE_INDEX_NAME\"] = \"sino-hr-chatbot\" #\"namfung-finance-chatbot\" # \n",
    "# end setting up credentials\"\"\"\n",
    "\n",
    "# setting up credentials\n",
    "os.environ[\"AZURE_COGNITIVE_SEARCH_SERVICE_NAME\"] = \"acs-testing-sunny\" # replace with yours search service name\n",
    "os.environ[\"AZURE_COGNITIVE_SEARCH_API_KEY\"] = \"oygYftyrBXiWoDLoZatDKNSLFttn9frM6DE4XlSb7kAzSeBR01eY\" # replace with your api key\n",
    "os.environ[\"AZURE_INDEX_NAME\"] = \"your-index-name\" \n",
    "# end setting up credentials\n",
    "\n",
    "# retriever = AzureCognitiveSearchRetriever(content_key=\"content\", top_k=10)\n",
    "\n",
    "def azure_search_by_index(question, index_name):\n",
    "\n",
    "    # set up openai environment\n",
    "    os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "    os.environ[\"OPENAI_API_BASE\"] = \"https://pwcjay.openai.azure.com/\"\n",
    "    os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"f282a661571f45a0bdfdcd295ac808e7\"\n",
    "\n",
    "    model: str = \"text-embedding-ada-002\"\n",
    "    search_service = os.environ[\"AZURE_COGNITIVE_SEARCH_SERVICE_NAME\"]\n",
    "    search_api_key = os.environ[\"AZURE_COGNITIVE_SEARCH_API_KEY\"]\n",
    "    vector_store_address: str = f\"https://{search_service}.search.windows.net\"\n",
    "    vector_store_password: str = search_api_key\n",
    "    \n",
    "\n",
    "    # define embedding model for calculating the embeddings\n",
    "    model: str = \"text-embedding-ada-002\"\n",
    "    embeddings: OpenAIEmbeddings = OpenAIEmbeddings(deployment=model, chunk_size=1)\n",
    "    embedding_function = embeddings.embed_query\n",
    "\n",
    "    # define schema of the json file stored on the index\n",
    "    fields = [\n",
    "            SimpleField(\n",
    "                name=\"id\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                key=True,\n",
    "                filterable=True,\n",
    "            ),\n",
    "            SearchableField(\n",
    "                name=\"content\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                searchable=True,\n",
    "            ),\n",
    "            SearchField(\n",
    "                name=\"content_vector\",\n",
    "                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True,\n",
    "                vector_search_dimensions=len(embedding_function(\"Text\")),\n",
    "                vector_search_configuration=\"default\",\n",
    "            ),\n",
    "            SearchableField(\n",
    "                name=\"metadata\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                searchable=True,\n",
    "            ),\n",
    "            # Additional field to store the title\n",
    "            SearchableField(\n",
    "                name=\"title\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                searchable=True,\n",
    "            ),\n",
    "            # Additional field for filtering on document source\n",
    "            SimpleField(\n",
    "                name=\"source\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                filterable=True,\n",
    "            ),\n",
    "            # Additional field for filtering on document source\n",
    "            SimpleField(\n",
    "                name=\"page\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                filterable=True,\n",
    "            ),\n",
    "            # Additional field for filtering on document source\n",
    "            SimpleField(\n",
    "                name=\"website_url\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                filterable=True,\n",
    "            ),\n",
    "        ]    \n",
    "    \n",
    "    vector_store: AzureSearch = AzureSearch(\n",
    "        azure_search_endpoint=vector_store_address,\n",
    "        azure_search_key=vector_store_password,\n",
    "        index_name=index_name,\n",
    "        embedding_function=embedding_function,\n",
    "        fields=fields,\n",
    "    )\n",
    "\n",
    "    relevant_documentation = vector_store.similarity_search(query=question, k=1, search_type=\"similarity\")\n",
    "    \n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_documentation])[:10000]\n",
    "\n",
    "    #lang = detect(question)\n",
    "\n",
    "    # change to lingua\n",
    "    languages = [Language.ENGLISH, Language.CHINESE]\n",
    "    detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "    lang = detector.detect_language_of(question)\n",
    "\n",
    "\n",
    "    #print(doc)\n",
    "    #print(context)\n",
    "    #print(relevant_documentation)\n",
    "    source = relevant_documentation[0].metadata['source']\n",
    "    #page_no = relevant_documentation[0].metadata['page']\n",
    "    website_url = relevant_documentation[0].metadata['website_url']\n",
    "    \n",
    "    page_no = \"\"\n",
    "    for doc in relevant_documentation:\n",
    "        page_no = page_no + \",\" + doc.metadata['page'] \n",
    "    \n",
    "    #print(relevant_documentation[0])\n",
    "    #print(source)\n",
    "    #print(page_no)\n",
    "    #print(website_url)\n",
    "    #return str(context), source, website_url, lang, page_no\n",
    "    # just return 10 documents (i.e. pages) if number of pages return from the search result > 10\n",
    "    if len(relevant_documentation) > 10:\n",
    "        relevant_documentation = relevant_documentation[0:9]\n",
    "    else:\n",
    "        relevant_documentation = relevant_documentation\n",
    "    return relevant_documentation, source, website_url, lang, page_no\n",
    "\n",
    "def generate_prompt():\n",
    "    prompt_template_string=\"\"\"\n",
    "    Follow exactly these 5 steps:\n",
    "    1. Read the context below and aggregrate this data\n",
    "    Context : {context}\n",
    "    2. Answer the question using only this context\n",
    "    3. Answer the question in less than 200 words\n",
    "    4. Please provide your answer in English\n",
    "    5. Please provide the page number of the pages where your answer are based on at the end of your response\n",
    "    6. Please provide the page numbers in the following output format: [Page: 1, 2, 3]\n",
    "    User Question: {question}\n",
    "\n",
    "    Don't justify your answers. Don't give information not mentioned in the given context\n",
    "    \n",
    "    If you don't have any context and are unsure of the answer, reply that you don't know about this topic.\n",
    "\n",
    "    Please provide your answer in English\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(template = prompt_template_string, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def generate_prompt_chi():\n",
    "    prompt_template_string=\"\"\"\n",
    "    指令：\n",
    "    1. 你必須只根據以下文字的內容回答提問者的詢問。\n",
    "    2. 如果不懂得回答或文字沒有資料，請回答“對不起，我不懂得回答這個問題。”\n",
    "    3. 請以少於200字回答問題。\n",
    "    4. 請在你的回答後提供你用以回答的文字的頁數。格式範例：[Page: 1, 2, 3]\n",
    "    5. 請以繁體中文回答。\n",
    "\n",
    "    #####\n",
    "    文本：{context}\n",
    "    #####\n",
    "\n",
    "    問題：{question}\n",
    "\n",
    "    請以繁體中文回答\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(template = prompt_template_string, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "# helper function to extract page number\n",
    "def extract_page_no(string):\n",
    "    if \"[Page\" in string:\n",
    "        print(re.findall('\\[Page:.*\\]', string)[0].split('Page:')[1])\n",
    "        return re.findall('\\[Page:.*\\]', string)[0].split(':')[1].split(\"]\")[0].strip()\n",
    "    elif \"(Page:\" in string: # handling for exception\n",
    "        print(re.findall('\\(Page:.*\\)', string)[0].split('Page:')[1])\n",
    "        return re.findall('\\(Page:.*\\)', string)[0].split(':')[1].split(\")\")[0].strip()\n",
    "    else:\n",
    "        return \"/\"\n",
    "\n",
    "def extract_answer(string):\n",
    "    if \"[Page\" in string:\n",
    "        return string.split(\"[Page\")[0]\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "def llm_pipeline(question):\n",
    "\n",
    "    # set up index name \n",
    "    index_name = os.environ[\"AZURE_INDEX_NAME\"] \n",
    "\n",
    "    # retrieve information from Azure Search\n",
    "    relevant_docs, source, website_url, lang, page_no = azure_search_by_index(question, index_name)\n",
    "\n",
    "    #print(relevant_docs)\n",
    "\n",
    "    # generate prompt without example\n",
    "\n",
    "    '''\n",
    "    if language == \"zh-cn\" or language == \"zh-tw\":\n",
    "        PROMPT = generate_prompt_chi()\n",
    "    #if language == \"en\":\n",
    "    #english prompt\n",
    "    #    PROMPT = generate_prompt()\n",
    "    else:\n",
    "    #english prompt\n",
    "        PROMPT = generate_prompt()\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # use lingua instead\n",
    "    if lang == Language.CHINESE:\n",
    "        PROMPT = generate_prompt_chi()\n",
    "        language = \"chinese\"\n",
    "    else:\n",
    "        PROMPT = generate_prompt()\n",
    "        language = \"english\"\n",
    "\n",
    "    os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "    os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "    os.environ[\"OPENAI_API_BASE\"] = \"https://pwcjay.openai.azure.com/\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"f282a661571f45a0bdfdcd295ac808e7\"\n",
    "\n",
    "    # use AzureChatOpenAI \n",
    "    llm = AzureChatOpenAI(deployment_name=\"gpt-35-16k\", temperature=0,\n",
    "                        openai_api_version=\"2023-05-15\", openai_api_base=\"https://pwcjay.openai.azure.com/\")\n",
    "\n",
    "    chain = LLMChain(llm=llm, \n",
    "                    prompt=PROMPT,\n",
    "                    #verbose=True\n",
    "                    )\n",
    "\n",
    "    output = chain.run({\"context\": relevant_docs, #\"context\": relevant_docs, \n",
    "        \"question\": question,\n",
    "        })\n",
    "\n",
    "    # wrapped_text = textwrap.fill(output, width=100)\n",
    "    # print(wrapped_text)\n",
    "\n",
    "    logging.info(output)\n",
    "\n",
    "    page_no = extract_page_no(output)\n",
    "    answer = extract_answer(output)\n",
    "\n",
    "    answer = answer #+ \"\\n\" + f\"[page:{page_no}]\"\n",
    "\n",
    "    # if no page number, then the source should be - and website url should be /\n",
    "\n",
    "    logging.info(page_no)\n",
    "\n",
    "    if page_no == \"/\" or page_no == \"N/A\":\n",
    "        source = \"-\"\n",
    "        website_url = \"/\"\n",
    "\n",
    "    # extract first page number\n",
    "    if \",\" in page_no:\n",
    "        first_page_no  = page_no.split(\",\")[0]\n",
    "    else:\n",
    "        first_page_no = page_no\n",
    "\n",
    "    #if language == \"en\":\n",
    "    #    source = \"Title: \" + source\n",
    "    #    page_no = \"Page: \" + page_no\n",
    "    #else:\n",
    "    #    source = \"文本来源: \" + source\n",
    "    #    page_no = \"页数: \" + page_no\n",
    "\n",
    "    json_response = {\n",
    "        \"raw\": output,\n",
    "        \"answer\": answer,\n",
    "        \"source\": source,\n",
    "        \"website_url\": website_url,\n",
    "        \"page_no\": page_no,\n",
    "        \"first_page_no\": first_page_no,\n",
    "        \"language\": language\n",
    "    }\n",
    "    return json_response #textwrap.fill(question, width=100), textwrap.fill(output, width=100)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4457a8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'raw': 'The Pharmacokinetic Measures of Systemic Exposure refer to the methods used to evaluate the bioavailability (BA) and bioequivalence (BE) of a drug in the body. These measures involve assessing the concentration of the drug in the plasma, serum, or blood over a specific period of time. \\n\\nThere are two main measures of systemic exposure mentioned in the context. The first measure is Peak Exposure, which involves assessing the maximum concentration (Cmax) of the drug directly from the concentration data without interpolation. The time at which the Cmax occurs, known as Tmax, provides information about the rate of absorption. It is recommended to collect early time points after dosing to accurately determine the Cmax.\\n\\nThe second measure is Total Exposure or Extent of Absorption, which involves calculating the area under the plasma, serum, or blood concentration-time curve from time zero to a specific time point (AUC0-t). Additionally, the AUC from time zero to infinity (AUC0-∞) is calculated by adding the last measurable drug concentration (Ct) divided by the terminal or elimination rate constant (λz). This measure provides information about the overall exposure to the drug.\\n\\nThese pharmacokinetic measures of systemic exposure are important in assessing the BA and BE of a drug and determining its efficacy and safety. [Page: 12]',\n",
       " 'answer': 'The Pharmacokinetic Measures of Systemic Exposure refer to the methods used to evaluate the bioavailability (BA) and bioequivalence (BE) of a drug in the body. These measures involve assessing the concentration of the drug in the plasma, serum, or blood over a specific period of time. \\n\\nThere are two main measures of systemic exposure mentioned in the context. The first measure is Peak Exposure, which involves assessing the maximum concentration (Cmax) of the drug directly from the concentration data without interpolation. The time at which the Cmax occurs, known as Tmax, provides information about the rate of absorption. It is recommended to collect early time points after dosing to accurately determine the Cmax.\\n\\nThe second measure is Total Exposure or Extent of Absorption, which involves calculating the area under the plasma, serum, or blood concentration-time curve from time zero to a specific time point (AUC0-t). Additionally, the AUC from time zero to infinity (AUC0-∞) is calculated by adding the last measurable drug concentration (Ct) divided by the terminal or elimination rate constant (λz). This measure provides information about the overall exposure to the drug.\\n\\nThese pharmacokinetic measures of systemic exposure are important in assessing the BA and BE of a drug and determining its efficacy and safety. ',\n",
       " 'source': 'Bioavailability-and-Bioequivalence-Studies-Submitted-in-NDAs-or-INDs-—-General-Considerations.pdf',\n",
       " 'website_url': '/',\n",
       " 'page_no': '12',\n",
       " 'first_page_no': '12',\n",
       " 'language': 'english'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What was the Pharmacokinetic Measures of Systemic Exposure?\"\n",
    "\n",
    "llm_pipeline(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cce878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
